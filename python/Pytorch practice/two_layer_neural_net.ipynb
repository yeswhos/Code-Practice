{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import operator\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2012, 0.7006, 0.0685],\n",
      "        [0.8754, 0.0235, 0.1121],\n",
      "        [0.8929, 0.1105, 0.0917],\n",
      "        [0.0814, 0.0635, 0.8166],\n",
      "        [0.9105, 0.9210, 0.0853]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype = torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4262, 1.0661, 1.0662],\n",
       "        [1.6769, 1.0176, 1.9241],\n",
       "        [1.8122, 1.8169, 1.8032],\n",
       "        [1.3747, 1.8122, 1.2546],\n",
       "        [1.7174, 1.9286, 1.4377]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "result = torch.empty(5, 3)\n",
    "torch.add (x, y, out = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0202, -1.1443, -0.1051,  2.2824,  0.3574, -0.1758, -0.8805,  1.7793],\n",
       "        [ 1.0300, -0.0786, -0.3269,  0.2332,  0.0331,  0.1424,  0.3752,  0.3183]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device = device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0201975 , -1.1442864 , -0.10506658,  2.2824323 ,  0.3573812 ,\n",
       "       -0.17584214, -0.88049424,  1.7792723 ,  1.0299932 , -0.07858746,\n",
       "       -0.32689717,  0.23323706,  0.03314139,  0.1423604 ,  0.3752281 ,\n",
       "        0.31828701], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor可以在gpu上，但numpy只有cpu，所以一般都是np转为tensor放到gpu运算，然后再转回numpy并放到cpu里\n",
    "y.to(\"cpu\").data.numpy()\n",
    "y.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用numpy实现两层神经网络\n",
    "----------------\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss\n",
    "- h = W_1X + b_1\n",
    "- a = max (0, h) ReLU\n",
    "- y_{hat} = W_2a + b_2\n",
    "\n",
    "实现\n",
    "- forward pass\n",
    "- loss\n",
    "- backward pass\n",
    "\n",
    "numpy ndarray只是一个普通的n维array，并不涉及任何深度学习或者梯度gradient的知识。只是一种用来计算数学运算的数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37330079.9821741\n",
      "1 31141540.215710815\n",
      "2 28895152.73736138\n",
      "3 25755639.577937238\n",
      "4 20276683.309540555\n",
      "5 13961857.319894487\n",
      "6 8637280.106683142\n",
      "7 5148183.258450883\n",
      "8 3140292.6277650776\n",
      "9 2049105.3749066382\n",
      "10 1445036.9350677927\n",
      "11 1091533.661590245\n",
      "12 866827.2291749764\n",
      "13 712188.8905861593\n",
      "14 598114.3194623955\n",
      "15 509185.82570199104\n",
      "16 437904.52453823196\n",
      "17 379485.29022308456\n",
      "18 330720.57711180503\n",
      "19 289628.576905093\n",
      "20 254734.85746594996\n",
      "21 224932.21133046155\n",
      "22 199274.61230761284\n",
      "23 177080.16045152151\n",
      "24 157808.08275309845\n",
      "25 141028.7806726255\n",
      "26 126345.90107098638\n",
      "27 113441.09583735152\n",
      "28 102065.27420329615\n",
      "29 92010.4478221159\n",
      "30 83098.91305846235\n",
      "31 75190.52280873308\n",
      "32 68157.42504827632\n",
      "33 61875.923908946854\n",
      "34 56261.48556820596\n",
      "35 51228.48500897418\n",
      "36 46708.78706976758\n",
      "37 42641.244425472934\n",
      "38 38976.83110366805\n",
      "39 35670.40327172875\n",
      "40 32683.089063409698\n",
      "41 29979.16442706629\n",
      "42 27527.978337167668\n",
      "43 25301.599765721796\n",
      "44 23277.07860867127\n",
      "45 21435.1011697373\n",
      "46 19756.95286878866\n",
      "47 18224.88722130633\n",
      "48 16824.870761614373\n",
      "49 15544.203800024698\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(it, loss)\n",
    "    \n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在pytorch上再实现一遍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26721898.0\n",
      "1 23825340.0\n",
      "2 26743670.0\n",
      "3 31681644.0\n",
      "4 34588864.0\n",
      "5 30907394.0\n",
      "6 21719118.0\n",
      "7 11979791.0\n",
      "8 5813523.0\n",
      "9 2808557.25\n",
      "10 1527440.25\n",
      "11 972467.6875\n",
      "12 707769.0\n",
      "13 560086.875\n",
      "14 463833.15625\n",
      "15 393350.03125\n",
      "16 338012.8125\n",
      "17 292901.25\n",
      "18 255313.5\n",
      "19 223614.59375\n",
      "20 196629.171875\n",
      "21 173497.640625\n",
      "22 153576.8125\n",
      "23 136341.6875\n",
      "24 121376.515625\n",
      "25 108329.4765625\n",
      "26 96915.40625\n",
      "27 86900.484375\n",
      "28 78078.0859375\n",
      "29 70280.5234375\n",
      "30 63374.34765625\n",
      "31 57244.2734375\n",
      "32 51794.13671875\n",
      "33 46938.29296875\n",
      "34 42597.2890625\n",
      "35 38709.98046875\n",
      "36 35220.8984375\n",
      "37 32083.5078125\n",
      "38 29257.837890625\n",
      "39 26707.953125\n",
      "40 24404.84765625\n",
      "41 22322.162109375\n",
      "42 20435.26953125\n",
      "43 18724.5078125\n",
      "44 17172.291015625\n",
      "45 15761.6640625\n",
      "46 14477.662109375\n",
      "47 13308.326171875\n",
      "48 12243.1552734375\n",
      "49 11270.3984375\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "# x = np.random.randn(N, D_in)\n",
    "# y = np.random.randn(N, D_out)\n",
    "\n",
    "# w1 = np.random.randn(D_in, H)\n",
    "# w2 = np.random.randn(H, D_out)\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "    #h = x.dot(w1)\n",
    "    #matrix mulitply\n",
    "    h = x.mm(w1)\n",
    "    #h_relu = np.maximum(h, 0)\n",
    "    h_relu = h.clamp(min = 0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    #loss = np.square(y_pred - y).sum()\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(it, loss)\n",
    "    \n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad = True)\n",
    "w = torch.tensor(2., requires_grad = True)\n",
    "b = torch.tensor(3., requires_grad = True)\n",
    "\n",
    "y = w * x + b # y = 2 * 1 + 3\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再进行简化之后的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad.zero_()\n",
    "w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33126524.0\n",
      "1 32853148.0\n",
      "2 37433264.0\n",
      "3 39726992.0\n",
      "4 34150600.0\n",
      "5 22079134.0\n",
      "6 11144883.0\n",
      "7 5134465.0\n",
      "8 2607425.0\n",
      "9 1609302.875\n",
      "10 1168911.75\n",
      "11 931114.8125\n",
      "12 774803.5\n",
      "13 658245.4375\n",
      "14 565665.75\n",
      "15 489707.0625\n",
      "16 426358.96875\n",
      "17 372965.125\n",
      "18 327607.125\n",
      "19 288879.8125\n",
      "20 255635.4375\n",
      "21 226957.328125\n",
      "22 201907.84375\n",
      "23 180152.75\n",
      "24 161198.9375\n",
      "25 144603.59375\n",
      "26 130012.1484375\n",
      "27 117155.3828125\n",
      "28 105773.84375\n",
      "29 95674.765625\n",
      "30 86689.8671875\n",
      "31 78677.3515625\n",
      "32 71520.703125\n",
      "33 65114.27734375\n",
      "34 59356.875\n",
      "35 54205.24609375\n",
      "36 49587.8359375\n",
      "37 45418.60546875\n",
      "38 41649.375\n",
      "39 38236.63671875\n",
      "40 35141.9453125\n",
      "41 32330.95703125\n",
      "42 29772.37890625\n",
      "43 27441.37890625\n",
      "44 25317.513671875\n",
      "45 23377.189453125\n",
      "46 21602.2265625\n",
      "47 19979.7734375\n",
      "48 18492.9921875\n",
      "49 17129.4609375\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, requires_grad = True)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(it, loss.item())\n",
    "    \n",
    "\n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch:nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34791804.0\n",
      "1 30468078.0\n",
      "2 29244070.0\n",
      "3 26309362.0\n",
      "4 20529724.0\n",
      "5 13640735.0\n",
      "6 8082017.0\n",
      "7 4613452.0\n",
      "8 2736262.0\n",
      "9 1762712.75\n",
      "10 1241866.75\n",
      "11 941071.5625\n",
      "12 749775.6875\n",
      "13 616923.25\n",
      "14 518124.0\n",
      "15 440983.40625\n",
      "16 378822.09375\n",
      "17 327739.9375\n",
      "18 285153.625\n",
      "19 249329.671875\n",
      "20 218922.28125\n",
      "21 192974.875\n",
      "22 170758.46875\n",
      "23 151600.921875\n",
      "24 135005.96875\n",
      "25 120582.1640625\n",
      "26 107979.0\n",
      "27 96945.296875\n",
      "28 87244.7265625\n",
      "29 78683.2109375\n",
      "30 71101.4375\n",
      "31 64373.5234375\n",
      "32 58389.11328125\n",
      "33 53052.5859375\n",
      "34 48286.9375\n",
      "35 44019.609375\n",
      "36 40191.24609375\n",
      "37 36748.21484375\n",
      "38 33646.203125\n",
      "39 30849.09375\n",
      "40 28323.447265625\n",
      "41 26037.79296875\n",
      "42 23965.669921875\n",
      "43 22083.193359375\n",
      "44 20372.12109375\n",
      "45 18814.517578125\n",
      "46 17392.083984375\n",
      "47 16092.6171875\n",
      "48 14904.8330078125\n",
      "49 13819.5751953125\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias = False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias = False)\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "#     y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    model.zero_grad()\n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "#         w1 -= learning_rate * w1.grad\n",
    "#         w2 -= learning_rate * w2.grad\n",
    "#         w1.grad.zero_()\n",
    "#         w2.grad.zero_()\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1000, out_features=100, bias=False)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch:optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 570.1884765625\n",
      "1 570.0304565429688\n",
      "2 569.8724365234375\n",
      "3 569.7144775390625\n",
      "4 569.5565185546875\n",
      "5 569.3986206054688\n",
      "6 569.24072265625\n",
      "7 569.0830078125\n",
      "8 568.9251708984375\n",
      "9 568.7674560546875\n",
      "10 568.6097412109375\n",
      "11 568.4521484375\n",
      "12 568.2945556640625\n",
      "13 568.1370849609375\n",
      "14 567.9796142578125\n",
      "15 567.822265625\n",
      "16 567.6649169921875\n",
      "17 567.5076904296875\n",
      "18 567.3504028320312\n",
      "19 567.1931762695312\n",
      "20 567.0359497070312\n",
      "21 566.8788452148438\n",
      "22 566.7218017578125\n",
      "23 566.5648193359375\n",
      "24 566.4078369140625\n",
      "25 566.2508544921875\n",
      "26 566.093994140625\n",
      "27 565.9371337890625\n",
      "28 565.7803955078125\n",
      "29 565.6239013671875\n",
      "30 565.467529296875\n",
      "31 565.3112182617188\n",
      "32 565.1549682617188\n",
      "33 564.9987182617188\n",
      "34 564.8425903320312\n",
      "35 564.6864013671875\n",
      "36 564.5302734375\n",
      "37 564.3742065429688\n",
      "38 564.2181396484375\n",
      "39 564.0621337890625\n",
      "40 563.9061279296875\n",
      "41 563.7501831054688\n",
      "42 563.59423828125\n",
      "43 563.4384155273438\n",
      "44 563.28271484375\n",
      "45 563.1270141601562\n",
      "46 562.971435546875\n",
      "47 562.81591796875\n",
      "48 562.6604614257812\n",
      "49 562.5050659179688\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias = False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias = False)\n",
    ")\n",
    "\n",
    "# torch.nn.init.normal_(model[0].weight)\n",
    "# torch.nn.init.normal_(model[2].weight)\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction = 'sum')\n",
    "#优化。有SGD，Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "#     y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "#     with torch.no_grad():\n",
    "# #         w1 -= learning_rate * w1.grad\n",
    "# #         w2 -= learning_rate * w2.grad\n",
    "# #         w1.grad.zero_()\n",
    "# #         w2.grad.zero_()\n",
    "#         for param in model.parameters():\n",
    "#             param -= learning_rate * param.grad\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 750.524169921875\n",
      "1 750.33203125\n",
      "2 750.14013671875\n",
      "3 749.9481201171875\n",
      "4 749.7562255859375\n",
      "5 749.5643310546875\n",
      "6 749.3724365234375\n",
      "7 749.1805419921875\n",
      "8 748.9887084960938\n",
      "9 748.7969970703125\n",
      "10 748.6053466796875\n",
      "11 748.4137573242188\n",
      "12 748.22216796875\n",
      "13 748.0308227539062\n",
      "14 747.8395385742188\n",
      "15 747.6482543945312\n",
      "16 747.4571533203125\n",
      "17 747.2659301757812\n",
      "18 747.0747680664062\n",
      "19 746.8836669921875\n",
      "20 746.692626953125\n",
      "21 746.5015258789062\n",
      "22 746.3104858398438\n",
      "23 746.1195678710938\n",
      "24 745.9286499023438\n",
      "25 745.73779296875\n",
      "26 745.5469970703125\n",
      "27 745.356201171875\n",
      "28 745.1655883789062\n",
      "29 744.9749145507812\n",
      "30 744.7843627929688\n",
      "31 744.5938720703125\n",
      "32 744.4033813476562\n",
      "33 744.2129516601562\n",
      "34 744.0224609375\n",
      "35 743.8320922851562\n",
      "36 743.6417236328125\n",
      "37 743.4514770507812\n",
      "38 743.2611694335938\n",
      "39 743.0709228515625\n",
      "40 742.880859375\n",
      "41 742.69091796875\n",
      "42 742.5009765625\n",
      "43 742.3109741210938\n",
      "44 742.12109375\n",
      "45 741.9312133789062\n",
      "46 741.7413940429688\n",
      "47 741.5516357421875\n",
      "48 741.3621215820312\n",
      "49 741.1724853515625\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H, bias = False)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(H, D_out, bias = False)\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear1(x)\n",
    "        y_pred = self.relu(y_pred)\n",
    "        y_pred = self.linear2(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "# torch.nn.init.normal_(model[0].weight)\n",
    "# torch.nn.init.normal_(model[2].weight)\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction = 'sum')\n",
    "#优化。有SGD，Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "#     y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "#     with torch.no_grad():\n",
    "# #         w1 -= learning_rate * w1.grad\n",
    "# #         w2 -= learning_rate * w2.grad\n",
    "# #         w1.grad.zero_()\n",
    "# #         w2.grad.zero_()\n",
    "#         for param in model.parameters():\n",
    "#             param -= learning_rate * param.grad\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
