{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import operator\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2012, 0.7006, 0.0685],\n",
      "        [0.8754, 0.0235, 0.1121],\n",
      "        [0.8929, 0.1105, 0.0917],\n",
      "        [0.0814, 0.0635, 0.8166],\n",
      "        [0.9105, 0.9210, 0.0853]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype = torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4262, 1.0661, 1.0662],\n",
       "        [1.6769, 1.0176, 1.9241],\n",
       "        [1.8122, 1.8169, 1.8032],\n",
       "        [1.3747, 1.8122, 1.2546],\n",
       "        [1.7174, 1.9286, 1.4377]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "result = torch.empty(5, 3)\n",
    "torch.add (x, y, out = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0202, -1.1443, -0.1051,  2.2824,  0.3574, -0.1758, -0.8805,  1.7793],\n",
       "        [ 1.0300, -0.0786, -0.3269,  0.2332,  0.0331,  0.1424,  0.3752,  0.3183]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device = device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0201975 , -1.1442864 , -0.10506658,  2.2824323 ,  0.3573812 ,\n",
       "       -0.17584214, -0.88049424,  1.7792723 ,  1.0299932 , -0.07858746,\n",
       "       -0.32689717,  0.23323706,  0.03314139,  0.1423604 ,  0.3752281 ,\n",
       "        0.31828701], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor可以在gpu上，但numpy只有cpu，所以一般都是np转为tensor放到gpu运算，然后再转回numpy并放到cpu里\n",
    "y.to(\"cpu\").data.numpy()\n",
    "y.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用numpy实现两层神经网络\n",
    "----------------\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss\n",
    "- h = W_1X + b_1\n",
    "- a = max (0, h) ReLU\n",
    "- y_{hat} = W_2a + b_2\n",
    "\n",
    "实现\n",
    "- forward pass\n",
    "- loss\n",
    "- backward pass\n",
    "\n",
    "numpy ndarray只是一个普通的n维array，并不涉及任何深度学习或者梯度gradient的知识。只是一种用来计算数学运算的数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37330079.9821741\n",
      "1 31141540.215710815\n",
      "2 28895152.73736138\n",
      "3 25755639.577937238\n",
      "4 20276683.309540555\n",
      "5 13961857.319894487\n",
      "6 8637280.106683142\n",
      "7 5148183.258450883\n",
      "8 3140292.6277650776\n",
      "9 2049105.3749066382\n",
      "10 1445036.9350677927\n",
      "11 1091533.661590245\n",
      "12 866827.2291749764\n",
      "13 712188.8905861593\n",
      "14 598114.3194623955\n",
      "15 509185.82570199104\n",
      "16 437904.52453823196\n",
      "17 379485.29022308456\n",
      "18 330720.57711180503\n",
      "19 289628.576905093\n",
      "20 254734.85746594996\n",
      "21 224932.21133046155\n",
      "22 199274.61230761284\n",
      "23 177080.16045152151\n",
      "24 157808.08275309845\n",
      "25 141028.7806726255\n",
      "26 126345.90107098638\n",
      "27 113441.09583735152\n",
      "28 102065.27420329615\n",
      "29 92010.4478221159\n",
      "30 83098.91305846235\n",
      "31 75190.52280873308\n",
      "32 68157.42504827632\n",
      "33 61875.923908946854\n",
      "34 56261.48556820596\n",
      "35 51228.48500897418\n",
      "36 46708.78706976758\n",
      "37 42641.244425472934\n",
      "38 38976.83110366805\n",
      "39 35670.40327172875\n",
      "40 32683.089063409698\n",
      "41 29979.16442706629\n",
      "42 27527.978337167668\n",
      "43 25301.599765721796\n",
      "44 23277.07860867127\n",
      "45 21435.1011697373\n",
      "46 19756.95286878866\n",
      "47 18224.88722130633\n",
      "48 16824.870761614373\n",
      "49 15544.203800024698\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(it, loss)\n",
    "    \n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
