{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import operator\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2012, 0.7006, 0.0685],\n",
      "        [0.8754, 0.0235, 0.1121],\n",
      "        [0.8929, 0.1105, 0.0917],\n",
      "        [0.0814, 0.0635, 0.8166],\n",
      "        [0.9105, 0.9210, 0.0853]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype = torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4262, 1.0661, 1.0662],\n",
       "        [1.6769, 1.0176, 1.9241],\n",
       "        [1.8122, 1.8169, 1.8032],\n",
       "        [1.3747, 1.8122, 1.2546],\n",
       "        [1.7174, 1.9286, 1.4377]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "result = torch.empty(5, 3)\n",
    "torch.add (x, y, out = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0202, -1.1443, -0.1051,  2.2824,  0.3574, -0.1758, -0.8805,  1.7793],\n",
       "        [ 1.0300, -0.0786, -0.3269,  0.2332,  0.0331,  0.1424,  0.3752,  0.3183]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device = device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0201975 , -1.1442864 , -0.10506658,  2.2824323 ,  0.3573812 ,\n",
       "       -0.17584214, -0.88049424,  1.7792723 ,  1.0299932 , -0.07858746,\n",
       "       -0.32689717,  0.23323706,  0.03314139,  0.1423604 ,  0.3752281 ,\n",
       "        0.31828701], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor可以在gpu上，但numpy只有cpu，所以一般都是np转为tensor放到gpu运算，然后再转回numpy并放到cpu里\n",
    "y.to(\"cpu\").data.numpy()\n",
    "y.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用numpy实现两层神经网络\n",
    "----------------\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss\n",
    "- h = W_1X + b_1\n",
    "- a = max (0, h) ReLU\n",
    "- y_{hat} = W_2a + b_2\n",
    "\n",
    "实现\n",
    "- forward pass\n",
    "- loss\n",
    "- backward pass\n",
    "\n",
    "numpy ndarray只是一个普通的n维array，并不涉及任何深度学习或者梯度gradient的知识。只是一种用来计算数学运算的数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37330079.9821741\n",
      "1 31141540.215710815\n",
      "2 28895152.73736138\n",
      "3 25755639.577937238\n",
      "4 20276683.309540555\n",
      "5 13961857.319894487\n",
      "6 8637280.106683142\n",
      "7 5148183.258450883\n",
      "8 3140292.6277650776\n",
      "9 2049105.3749066382\n",
      "10 1445036.9350677927\n",
      "11 1091533.661590245\n",
      "12 866827.2291749764\n",
      "13 712188.8905861593\n",
      "14 598114.3194623955\n",
      "15 509185.82570199104\n",
      "16 437904.52453823196\n",
      "17 379485.29022308456\n",
      "18 330720.57711180503\n",
      "19 289628.576905093\n",
      "20 254734.85746594996\n",
      "21 224932.21133046155\n",
      "22 199274.61230761284\n",
      "23 177080.16045152151\n",
      "24 157808.08275309845\n",
      "25 141028.7806726255\n",
      "26 126345.90107098638\n",
      "27 113441.09583735152\n",
      "28 102065.27420329615\n",
      "29 92010.4478221159\n",
      "30 83098.91305846235\n",
      "31 75190.52280873308\n",
      "32 68157.42504827632\n",
      "33 61875.923908946854\n",
      "34 56261.48556820596\n",
      "35 51228.48500897418\n",
      "36 46708.78706976758\n",
      "37 42641.244425472934\n",
      "38 38976.83110366805\n",
      "39 35670.40327172875\n",
      "40 32683.089063409698\n",
      "41 29979.16442706629\n",
      "42 27527.978337167668\n",
      "43 25301.599765721796\n",
      "44 23277.07860867127\n",
      "45 21435.1011697373\n",
      "46 19756.95286878866\n",
      "47 18224.88722130633\n",
      "48 16824.870761614373\n",
      "49 15544.203800024698\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(it, loss)\n",
    "    \n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在pytorch上再实现一遍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26721898.0\n",
      "1 23825340.0\n",
      "2 26743670.0\n",
      "3 31681644.0\n",
      "4 34588864.0\n",
      "5 30907394.0\n",
      "6 21719118.0\n",
      "7 11979791.0\n",
      "8 5813523.0\n",
      "9 2808557.25\n",
      "10 1527440.25\n",
      "11 972467.6875\n",
      "12 707769.0\n",
      "13 560086.875\n",
      "14 463833.15625\n",
      "15 393350.03125\n",
      "16 338012.8125\n",
      "17 292901.25\n",
      "18 255313.5\n",
      "19 223614.59375\n",
      "20 196629.171875\n",
      "21 173497.640625\n",
      "22 153576.8125\n",
      "23 136341.6875\n",
      "24 121376.515625\n",
      "25 108329.4765625\n",
      "26 96915.40625\n",
      "27 86900.484375\n",
      "28 78078.0859375\n",
      "29 70280.5234375\n",
      "30 63374.34765625\n",
      "31 57244.2734375\n",
      "32 51794.13671875\n",
      "33 46938.29296875\n",
      "34 42597.2890625\n",
      "35 38709.98046875\n",
      "36 35220.8984375\n",
      "37 32083.5078125\n",
      "38 29257.837890625\n",
      "39 26707.953125\n",
      "40 24404.84765625\n",
      "41 22322.162109375\n",
      "42 20435.26953125\n",
      "43 18724.5078125\n",
      "44 17172.291015625\n",
      "45 15761.6640625\n",
      "46 14477.662109375\n",
      "47 13308.326171875\n",
      "48 12243.1552734375\n",
      "49 11270.3984375\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "# x = np.random.randn(N, D_in)\n",
    "# y = np.random.randn(N, D_out)\n",
    "\n",
    "# w1 = np.random.randn(D_in, H)\n",
    "# w2 = np.random.randn(H, D_out)\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "    #h = x.dot(w1)\n",
    "    #matrix mulitply\n",
    "    h = x.mm(w1)\n",
    "    #h_relu = np.maximum(h, 0)\n",
    "    h_relu = h.clamp(min = 0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    #loss = np.square(y_pred - y).sum()\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(it, loss)\n",
    "    \n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad = True)\n",
    "w = torch.tensor(2., requires_grad = True)\n",
    "b = torch.tensor(3., requires_grad = True)\n",
    "\n",
    "y = w * x + b # y = 2 * 1 + 3\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再进行简化之后的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad.zero_()\n",
    "w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33126524.0\n",
      "1 32853148.0\n",
      "2 37433264.0\n",
      "3 39726992.0\n",
      "4 34150600.0\n",
      "5 22079134.0\n",
      "6 11144883.0\n",
      "7 5134465.0\n",
      "8 2607425.0\n",
      "9 1609302.875\n",
      "10 1168911.75\n",
      "11 931114.8125\n",
      "12 774803.5\n",
      "13 658245.4375\n",
      "14 565665.75\n",
      "15 489707.0625\n",
      "16 426358.96875\n",
      "17 372965.125\n",
      "18 327607.125\n",
      "19 288879.8125\n",
      "20 255635.4375\n",
      "21 226957.328125\n",
      "22 201907.84375\n",
      "23 180152.75\n",
      "24 161198.9375\n",
      "25 144603.59375\n",
      "26 130012.1484375\n",
      "27 117155.3828125\n",
      "28 105773.84375\n",
      "29 95674.765625\n",
      "30 86689.8671875\n",
      "31 78677.3515625\n",
      "32 71520.703125\n",
      "33 65114.27734375\n",
      "34 59356.875\n",
      "35 54205.24609375\n",
      "36 49587.8359375\n",
      "37 45418.60546875\n",
      "38 41649.375\n",
      "39 38236.63671875\n",
      "40 35141.9453125\n",
      "41 32330.95703125\n",
      "42 29772.37890625\n",
      "43 27441.37890625\n",
      "44 25317.513671875\n",
      "45 23377.189453125\n",
      "46 21602.2265625\n",
      "47 19979.7734375\n",
      "48 18492.9921875\n",
      "49 17129.4609375\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, requires_grad = True)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(it, loss.item())\n",
    "    \n",
    "\n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch:nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 665.4327392578125\n",
      "1 664.96240234375\n",
      "2 664.4926147460938\n",
      "3 664.023681640625\n",
      "4 663.5553588867188\n",
      "5 663.0875244140625\n",
      "6 662.6201782226562\n",
      "7 662.1534423828125\n",
      "8 661.6875610351562\n",
      "9 661.2220458984375\n",
      "10 660.7565307617188\n",
      "11 660.2913818359375\n",
      "12 659.8267211914062\n",
      "13 659.3624267578125\n",
      "14 658.8985595703125\n",
      "15 658.4351806640625\n",
      "16 657.9722290039062\n",
      "17 657.5098876953125\n",
      "18 657.0491333007812\n",
      "19 656.5888671875\n",
      "20 656.12890625\n",
      "21 655.6694946289062\n",
      "22 655.2105102539062\n",
      "23 654.7522583007812\n",
      "24 654.2955322265625\n",
      "25 653.8392333984375\n",
      "26 653.3837280273438\n",
      "27 652.9287109375\n",
      "28 652.4749145507812\n",
      "29 652.021484375\n",
      "30 651.5693969726562\n",
      "31 651.1177368164062\n",
      "32 650.6668090820312\n",
      "33 650.2161865234375\n",
      "34 649.7660522460938\n",
      "35 649.3163452148438\n",
      "36 648.8671264648438\n",
      "37 648.4188842773438\n",
      "38 647.9712524414062\n",
      "39 647.5238037109375\n",
      "40 647.0768432617188\n",
      "41 646.6305541992188\n",
      "42 646.1845703125\n",
      "43 645.73974609375\n",
      "44 645.2957153320312\n",
      "45 644.8519897460938\n",
      "46 644.40869140625\n",
      "47 643.9657592773438\n",
      "48 643.5233154296875\n",
      "49 643.0811767578125\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias = False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias = False)\n",
    ")\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "#     y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    model.zero_grad()\n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "#         w1 -= learning_rate * w1.grad\n",
    "#         w2 -= learning_rate * w2.grad\n",
    "#         w1.grad.zero_()\n",
    "#         w2.grad.zero_()\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
