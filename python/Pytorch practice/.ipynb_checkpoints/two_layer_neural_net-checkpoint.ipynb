{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import operator\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2012, 0.7006, 0.0685],\n",
      "        [0.8754, 0.0235, 0.1121],\n",
      "        [0.8929, 0.1105, 0.0917],\n",
      "        [0.0814, 0.0635, 0.8166],\n",
      "        [0.9105, 0.9210, 0.0853]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype = torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4262, 1.0661, 1.0662],\n",
       "        [1.6769, 1.0176, 1.9241],\n",
       "        [1.8122, 1.8169, 1.8032],\n",
       "        [1.3747, 1.8122, 1.2546],\n",
       "        [1.7174, 1.9286, 1.4377]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "result = torch.empty(5, 3)\n",
    "torch.add (x, y, out = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0202, -1.1443, -0.1051,  2.2824,  0.3574, -0.1758, -0.8805,  1.7793],\n",
       "        [ 1.0300, -0.0786, -0.3269,  0.2332,  0.0331,  0.1424,  0.3752,  0.3183]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device = device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0201975 , -1.1442864 , -0.10506658,  2.2824323 ,  0.3573812 ,\n",
       "       -0.17584214, -0.88049424,  1.7792723 ,  1.0299932 , -0.07858746,\n",
       "       -0.32689717,  0.23323706,  0.03314139,  0.1423604 ,  0.3752281 ,\n",
       "        0.31828701], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor可以在gpu上，但numpy只有cpu，所以一般都是np转为tensor放到gpu运算，然后再转回numpy并放到cpu里\n",
    "y.to(\"cpu\").data.numpy()\n",
    "y.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用numpy实现两层神经网络\n",
    "----------------\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss\n",
    "- h = W_1X + b_1\n",
    "- a = max (0, h) ReLU\n",
    "- y_{hat} = W_2a + b_2\n",
    "\n",
    "实现\n",
    "- forward pass\n",
    "- loss\n",
    "- backward pass\n",
    "\n",
    "numpy ndarray只是一个普通的n维array，并不涉及任何深度学习或者梯度gradient的知识。只是一种用来计算数学运算的数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37330079.9821741\n",
      "1 31141540.215710815\n",
      "2 28895152.73736138\n",
      "3 25755639.577937238\n",
      "4 20276683.309540555\n",
      "5 13961857.319894487\n",
      "6 8637280.106683142\n",
      "7 5148183.258450883\n",
      "8 3140292.6277650776\n",
      "9 2049105.3749066382\n",
      "10 1445036.9350677927\n",
      "11 1091533.661590245\n",
      "12 866827.2291749764\n",
      "13 712188.8905861593\n",
      "14 598114.3194623955\n",
      "15 509185.82570199104\n",
      "16 437904.52453823196\n",
      "17 379485.29022308456\n",
      "18 330720.57711180503\n",
      "19 289628.576905093\n",
      "20 254734.85746594996\n",
      "21 224932.21133046155\n",
      "22 199274.61230761284\n",
      "23 177080.16045152151\n",
      "24 157808.08275309845\n",
      "25 141028.7806726255\n",
      "26 126345.90107098638\n",
      "27 113441.09583735152\n",
      "28 102065.27420329615\n",
      "29 92010.4478221159\n",
      "30 83098.91305846235\n",
      "31 75190.52280873308\n",
      "32 68157.42504827632\n",
      "33 61875.923908946854\n",
      "34 56261.48556820596\n",
      "35 51228.48500897418\n",
      "36 46708.78706976758\n",
      "37 42641.244425472934\n",
      "38 38976.83110366805\n",
      "39 35670.40327172875\n",
      "40 32683.089063409698\n",
      "41 29979.16442706629\n",
      "42 27527.978337167668\n",
      "43 25301.599765721796\n",
      "44 23277.07860867127\n",
      "45 21435.1011697373\n",
      "46 19756.95286878866\n",
      "47 18224.88722130633\n",
      "48 16824.870761614373\n",
      "49 15544.203800024698\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(it, loss)\n",
    "    \n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在pytorch上再实现一遍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26721898.0\n",
      "1 23825340.0\n",
      "2 26743670.0\n",
      "3 31681644.0\n",
      "4 34588864.0\n",
      "5 30907394.0\n",
      "6 21719118.0\n",
      "7 11979791.0\n",
      "8 5813523.0\n",
      "9 2808557.25\n",
      "10 1527440.25\n",
      "11 972467.6875\n",
      "12 707769.0\n",
      "13 560086.875\n",
      "14 463833.15625\n",
      "15 393350.03125\n",
      "16 338012.8125\n",
      "17 292901.25\n",
      "18 255313.5\n",
      "19 223614.59375\n",
      "20 196629.171875\n",
      "21 173497.640625\n",
      "22 153576.8125\n",
      "23 136341.6875\n",
      "24 121376.515625\n",
      "25 108329.4765625\n",
      "26 96915.40625\n",
      "27 86900.484375\n",
      "28 78078.0859375\n",
      "29 70280.5234375\n",
      "30 63374.34765625\n",
      "31 57244.2734375\n",
      "32 51794.13671875\n",
      "33 46938.29296875\n",
      "34 42597.2890625\n",
      "35 38709.98046875\n",
      "36 35220.8984375\n",
      "37 32083.5078125\n",
      "38 29257.837890625\n",
      "39 26707.953125\n",
      "40 24404.84765625\n",
      "41 22322.162109375\n",
      "42 20435.26953125\n",
      "43 18724.5078125\n",
      "44 17172.291015625\n",
      "45 15761.6640625\n",
      "46 14477.662109375\n",
      "47 13308.326171875\n",
      "48 12243.1552734375\n",
      "49 11270.3984375\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "# x = np.random.randn(N, D_in)\n",
    "# y = np.random.randn(N, D_out)\n",
    "\n",
    "# w1 = np.random.randn(D_in, H)\n",
    "# w2 = np.random.randn(H, D_out)\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "    #h = x.dot(w1)\n",
    "    #matrix mulitply\n",
    "    h = x.mm(w1)\n",
    "    #h_relu = np.maximum(h, 0)\n",
    "    h_relu = h.clamp(min = 0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    #loss = np.square(y_pred - y).sum()\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(it, loss)\n",
    "    \n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    #update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 = w2 - learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad = True)\n",
    "w = torch.tensor(2., requires_grad = True)\n",
    "b = torch.tensor(3., requires_grad = True)\n",
    "\n",
    "y = w * x + b # y = 2 * 1 + 3\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再进行简化之后的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(28129470., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'zero_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-a70117118e1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_'"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "#64个输入，1000维度，hidden是100维度，输出10维度\n",
    "#64个训练数据一个batch，把一个1000维度的向量转成一个10维度的向量\n",
    "\n",
    "#随机创建一些训练数据\n",
    "# x = np.random.randn(N, D_in)\n",
    "# y = np.random.randn(N, D_out)\n",
    "\n",
    "# w1 = np.random.randn(D_in, H)\n",
    "# w2 = np.random.randn(H, D_out)\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, requires_grad = True)\n",
    "\n",
    "learning_rate = math.pow(10, -6)\n",
    "\n",
    "for it in range(50):\n",
    "    #forward pass\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "    \n",
    "    #compute loss\n",
    "    #MSE mean square error\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(it, loss)\n",
    "    \n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "    #backward pass\n",
    "    #compute the gradient\n",
    "    loss.backward()\n",
    "    \n",
    "#     #update weights of w1 and w2\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 = w2 - learning_rate * w2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  482128.1562,  -145314.5781,  -109746.5781,  ...,\n",
       "           533428.0000,  -318306.3125,   -37360.2422],\n",
       "        [ -128335.5234,    21929.3926,  -217515.8750,  ...,\n",
       "          -765629.1250,   170016.1719,   101634.3672],\n",
       "        [  642537.1875,   -86364.7031,   536948.0625,  ...,\n",
       "           325047.0312,     1377.7100,   146515.7188],\n",
       "        ...,\n",
       "        [-1031909.3125,   -87247.6250,  -208587.4844,  ...,\n",
       "           958350.8750,  -227218.7344,  -212136.7031],\n",
       "        [  913759.3125,   -84992.9297,   -74438.6562,  ...,\n",
       "         -1159775.1250,   659314.6875,  -198528.1094],\n",
       "        [   -3321.8535,    39343.6836,  -243740.4688,  ...,\n",
       "           821477.1875,   145153.4531,  -216722.6406]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
